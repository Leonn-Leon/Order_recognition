# -*- coding: utf-8 -*-
"""LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0znCxlKMCtzcU1fQIQHYQ_sQEWHGBt0
"""

!pip install -q datasets peft
!pip install -U bitsandbytes

import json
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments,\
                            Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# 1) Загрузка датасета из JSONL
def load_dataset(path):
    prompts = []
    answers = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            entry = json.loads(line)
            system_text = entry["request"][0]["text"] if "system" in entry else ""
            question_text = entry["request"][1]["text"]
            answer_text = entry["response"]

            # Формируем входной промпт.
            # Можно добавлять любые символы-разделители, чтобы модель лучше понимала границу между system/question.
            prompt = f"<system>\n{system_text}\n</system>\n" \
                     f"<question>\n{question_text}\n</question>\n" \
                     f"<answer>\n"

            prompts.append(prompt)
            answers.append(answer_text)

    dataset = Dataset.from_dict({"prompt": prompts, "answer": answers})
    return dataset

# 2) Токенизация и формирование input_ids
def tokenize_function(example, tokenizer, max_length=512):
    # Склеиваем prompt и ответ так, чтобы модель обучалась генерировать 'answer'
    # Здесь подход «полный контекст -> вся разметка -> включая ответ в labels»
    # При этом в labels нам нужно «замаскировать» токены prompt, чтобы не штрафовать модель за то, что она
    # просто воспроизвела prompt. Для simplicity возьмём подход, при котором labels == input_ids,
    # но токены prompt помечены как -100 в labels.
    # Это классический подход для causal LM обучения с инструкциями.

    # Преобразуем prompt и answer в одну строку. Здесь явно добавляем разделитель.
    # Prompt отделяем от answer специальным символом, например "###".
    full_text = example["prompt"] + example["answer"]

    tokenized = tokenizer(
        full_text,
        truncation=True,
        max_length=max_length,
        padding="max_length"
    )

    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]

    # Ищем конец prompt (он заканчивается на "<answer>\n")
    # Хотя точный поиск может быть сложнее, используем простую длину prompt после токенизации.
    prompt_tokenized = tokenizer(
        example["prompt"], truncation=True, max_length=max_length, padding="max_length"
    )

    prompt_length = sum(
        1 for t in prompt_tokenized["input_ids"] if t not in [0, tokenizer.pad_token_id]
    )

    # Формируем labels: все токены до prompt_length = -100, т.е. не участвуют в функции потерь
    labels = [-100] * len(input_ids)
    for i in range(prompt_length, len(input_ids)):
        # Если токен не паддинг (0 или pad_token_id), то записываем в labels
        if input_ids[i] != tokenizer.pad_token_id:
            labels[i] = input_ids[i]

    tokenized["labels"] = labels

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

def main():
    # Шаг 1. Загружаем базовую модель t-tech/T-lite-it-1.0
    model_name = "t-tech/T-lite-it-1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # quantization_config = BitsAndBytesConfig(load_in_8bit=True)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto"
    )

    # Шаг 2. Готовим модель к обучению с помощью LoRA
    # Настраиваем LoRA для слоёв модели: выбираем нужные параметры
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj","v_proj"],  # Названия весов, которые хотим декомпозировать LoRA
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # Если модель загружена в 8-битном формате, нужно обернуть её в prepare_model_for_int8_training
    model = prepare_model_for_kbit_training(model)
    # Создаём PEFT-модель
    model = get_peft_model(model, lora_config)

    # Шаг 3. Загружаем ваш датасет
    train_data = load_dataset("/content/drive/MyDrive/NLP/FT_lora_AG.json")

    # Разделяем на train/test, если нужно (для примера оставим всё как train)
    # train_data, eval_data = train_data.train_test_split(test_size=0.1).values()
    # Или можно не делать split

    # Шаг 4. Токенизация датасета
    def token_map_fn(example):
        return tokenize_function(example, tokenizer, max_length=512)

    train_data = train_data.map(token_map_fn)

    # Шаг 5. Настраиваем Trainer
    training_args = TrainingArguments(
        output_dir="lora-t-lite-checkpoints",
        evaluation_strategy="no",          # Можно "steps" или "epoch" при наличии валидации
        per_device_train_batch_size=1,     # Подберите под вашу GPU
        gradient_accumulation_steps=4,    # Увеличивает эффективный batch size
        num_train_epochs=10,
        learning_rate=2e-4,
        fp16=True,                         # Использование 16-бит вычислений (если поддерживается)
        logging_steps=50,
        save_steps=200,
        save_total_limit=2,
        gradient_checkpointing=True
        # optim="paged_adamw_8bit"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        # eval_dataset=eval_data,  # если есть eval-датасет
    )

    # Шаг 6. Запуск обучения
    trainer.train()

    # Шаг 7. Сохранение
    # Т.к. у нас PEFT-модель, сохраняем адаптер LoRA
    model.save_pretrained("lora-t-lite-model")

    print("Готово! LoRA-адаптер сохранён в папке lora-t-lite-model.")


if __name__ == "__main__":
    main()

!cp -r lora-t-lite-model drive/MyDrive/NLP/

"""### GPU объединение"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_name = "t-tech/T-lite-it-1.0"
lora_path = "drive/MyDrive/NLP/lora-t-lite-model"
merged_model_path = "merged-t-lite-model"

print("Загружаем базовую модель...")
# Загружаем базовую модель полностью, без low_cpu_mem_usage и offload
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="cuda"
)

print("Подгружаем LoRA-адаптер...")
lora_model = PeftModel.from_pretrained(
    base_model,
    lora_path,
    device_map="cuda"
)

print("Сливаем LoRA с базовой моделью...")
merged_model = lora_model.merge_and_unload()

print(f"Сохраняем результат в папку: {merged_model_path}")
merged_model.save_pretrained(merged_model_path, safe_serialization=False)

print("Готово!")

"""### CPU объединение"""

import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel

base_model_name = "t-tech/T-lite-it-1.0"
lora_path = "drive/MyDrive/NLP/lora-t-lite-model"
merged_model_path = "merged-t-lite-model"

print("Загружаем базовую модель на CPU...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="cpu"
)

print("Загружаем LoRA-адаптер...")
lora_model = PeftModel.from_pretrained(
    base_model,
    lora_path,
    device_map="cpu"
)

print("Объединяем LoRA с базовой моделью на CPU...")
merged_model = lora_model.merge_and_unload()

print("Сохраняем объединённую модель...")
merged_model.save_pretrained(merged_model_path, safe_serialization=False)
print("Готово!")

!cp -r /content/merged-t-lite-model drive/MyDrive/NLP/

"""## Конвертация в GGUF"""

!git clone https://github.com/ggerganov/llama.cpp.git

!pip install -qr llama.cpp/requirements.txt

model_name = "t-tech/T-lite-it-1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.save_pretrained("merged-t-lite-model")

!python llama.cpp/convert_hf_to_gguf.py merged-t-lite-model/ \
    --outfile model.gguf \
    --outtype q8_0 \
    --use-temp-file

!rm -r lora-t-lite-checkpoints

!cp model.gguf drive/MyDrive/NLP/